import{_ as s,c as e,o as a,af as t}from"./chunks/framework.Cm3tdt7z.js";const k=JSON.parse('{"title":"Performance Optimization Guide","description":"","frontmatter":{},"headers":[],"relativePath":"更多/optimization.md","filePath":"更多/optimization.md"}'),l={name:"更多/optimization.md"};function n(o,i,r,h,p,d){return a(),e("div",null,[...i[0]||(i[0]=[t(`<h1 id="performance-optimization-guide" tabindex="-1">Performance Optimization Guide <a class="header-anchor" href="#performance-optimization-guide" aria-label="Permalink to “Performance Optimization Guide”">​</a></h1><blockquote><p>This guide provides performance optimization tips for running <strong>Webis</strong> on local or cloud servers. Since Webis is based on vLLM and large language models, <strong>GPU memory</strong> and <strong>CPU throughput</strong> greatly affect performance.</p></blockquote><h2 id="hardware-recommendations" tabindex="-1">Hardware Recommendations <a class="header-anchor" href="#hardware-recommendations" aria-label="Permalink to “Hardware Recommendations”">​</a></h2><ul><li><strong>GPU</strong>: NVIDIA GPU supporting CUDA <ul><li>Recommended: ≥ 8GB memory</li><li>Minimum: 6GB memory (need to reduce <code>--memory-limit</code> and precision)</li></ul></li><li><strong>CPU</strong>: ≥ 4 physical cores</li><li><strong>Memory</strong>: ≥ 16GB</li><li><strong>Hard Drive</strong>: SSD (speeds up model loading)</li></ul><h2 id="gpu-memory-optimization" tabindex="-1">GPU Memory Optimization <a class="header-anchor" href="#gpu-memory-optimization" aria-label="Permalink to “GPU Memory Optimization”">​</a></h2><ol><li><strong>Adjust Memory Utilization</strong><ul><li>By default, vLLM uses 90% of available memory</li><li>Can be reduced when memory is insufficient:</li></ul></li></ol><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> scripts/start_model_server.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --memory-limit</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.6</span></span></code></pre></div><ol start="2"><li><p><strong>Use Low Precision</strong></p><ul><li><code>float16</code>: Significantly reduces memory usage with minimal precision loss</li><li><code>int8</code> quantization: Further reduces memory but may affect results</li></ul><p>Example (modify model loading in <code>start_model_server.py</code>):</p></li></ol><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LLM(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model_path,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    tensor_parallel_size</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    gpu_memory_utilization</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gpu_memory_utilization,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;float16&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><ol start="3"><li><strong>Release Memory Before Starting</strong></li></ol><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nvidia-smi</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">kill</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -9</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">PI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">D</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span></code></pre></div><h2 id="cpu-and-concurrency-optimization" tabindex="-1">CPU and Concurrency Optimization <a class="header-anchor" href="#cpu-and-concurrency-optimization" aria-label="Permalink to “CPU and Concurrency Optimization”">​</a></h2><ol><li><strong>Increase Concurrent Requests</strong></li></ol><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">uvicorn</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> scripts.start_model_server:app</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --host</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.0.0.0</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --port</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8000</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --workers</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span></code></pre></div><ol start="2"><li><p><strong>Request Batching</strong></p><ul><li>vLLM supports batch processing of multiple prompts</li><li>Recommend merging small requests into one API call</li></ul></li><li><p><strong>Batch Processing</strong></p><ul><li>Use <code>batch_extract_text()</code> to process multiple files</li></ul></li><li><p><strong>Parallel Processing</strong></p><ul><li>For large numbers of files, consider multi-process parallel processing</li></ul></li></ol><h2 id="disk-and-model-loading" tabindex="-1">Disk and Model Loading <a class="header-anchor" href="#disk-and-model-loading" aria-label="Permalink to “Disk and Model Loading”">​</a></h2><ol><li><strong>Cache Models</strong> HuggingFace cache directory:</li></ol><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">~</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">/.cache/huggingface/hub</span></span></code></pre></div><ol start="2"><li><p><strong>Use SSD Storage</strong> Model loading speed is much faster than HDD</p></li><li><p><strong>Cache Results</strong></p><ul><li>For repeatedly processed files, it is recommended to cache the results</li></ul></li><li><p><strong>Lazy Loading</strong></p><ul><li>The image processor adopts lazy loading to avoid unnecessary model initialization</li></ul></li></ol><h2 id="network-optimization" tabindex="-1">Network Optimization <a class="header-anchor" href="#network-optimization" aria-label="Permalink to “Network Optimization”">​</a></h2><ol><li>Choose cloud servers in regions closest to users</li><li>Use reverse proxies like Nginx and enable HTTP keep-alive</li><li>Enable response compression (gzip)</li></ol><h2 id="common-optimization-scenarios" tabindex="-1">Common Optimization Scenarios <a class="header-anchor" href="#common-optimization-scenarios" aria-label="Permalink to “Common Optimization Scenarios”">​</a></h2><ul><li><p><strong>Low Memory GPU (6GB)</strong>:</p><ul><li><code>--memory-limit 0.6</code></li><li><code>dtype=&quot;float16&quot;</code></li><li>Reduce <code>max_tokens</code> in API requests</li></ul></li><li><p><strong>High Concurrency API</strong>:</p><ul><li>Increase <code>uvicorn</code> workers</li><li>Use batching</li></ul></li><li><p><strong>Slow Model Loading</strong>:</p><ul><li>Keep service running continuously, avoid frequent restarts</li><li>Put models on SSD</li></ul></li></ul><h2 id="troubleshooting" tabindex="-1">Troubleshooting <a class="header-anchor" href="#troubleshooting" aria-label="Permalink to “Troubleshooting”">​</a></h2><ul><li><strong>&quot;No available memory for cache blocks&quot;</strong>: Reduce <code>--memory-limit</code> or free up memory</li><li><strong>&quot;Free memory on device ... less than desired&quot;</strong>: Reduce memory utilization or close other GPU processes</li><li><strong>&quot;CUDA not found&quot;</strong>: Install NVIDIA drivers and CUDA toolkit</li></ul>`,25)])])}const g=s(l,[["render",n]]);export{k as __pageData,g as default};
